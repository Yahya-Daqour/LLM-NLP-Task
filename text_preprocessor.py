# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lduo0tMdiGSMpBqX2pJ8771_iSqHBegF
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")

print("Path to dataset files:", path)

import pandas as pd
import re

df = pd.read_csv(path + "/IMDB Dataset.csv")
df.head()

# Normalize the text by making all letters lowercase.
df["review"] = df["review"].str.lower()
df

# Remove all HTML tags (e.g., <br/>).
df['review'] = df['review'].str.replace(r'<[^<>]*>', '', regex=True)
df

# Remove all email addresses.
df['review'] = df['review'].str.replace(r'\S*@\S*\s?', '', regex=True)
df

# Remove all URLs.
df['review'] = df['review'].str.replace(r"www\.[^\s]+|http[^\s]+", "", regex=True)  # Matches URLs
df

# Remove all URLs.
df['review'] = df['review'].str.replace(r"www\.[^\s]+|http[^\s]+", "", regex=True)
df

# Remove all punctuation.
df['review'] = df['review'].str.replace(r"[^\w\s]", "", regex=True)
df

# Remove stop words
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

# Sample text
text = "This is a sample sentence showing stopword removal."

# Get English stopwords and tokenize
stop_words = set(stopwords.words('english'))
tokens = word_tokenize(text.lower())

# Remove stopwords
filtered_tokens = [word for word in tokens if word not in stop_words]

print("Original:", tokens)
print("Filtered:", filtered_tokens)

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download required NLTK data (only needed once)
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')


# Set of English stopwords
stop_words = set(stopwords.words('english'))

# Function to remove stopwords from a single sentence
def remove_stopwords(sentence):
    tokens = word_tokenize(sentence.lower())  # Tokenize and lowercase
    filtered = [word for word in tokens if word.isalnum() and word not in stop_words]
    return ' '.join(filtered)

# Apply to the 'text' column
df['review'] = df['review'].apply(remove_stopwords)

# View result
df

# Lemmatize the words.

nltk.download('wordnet')
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
     lemmatized = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]
     return ' '.join(lemmatized)

df['review'] = df.review.apply(lemmatize_text)

df.to_csv("tmp.csv")